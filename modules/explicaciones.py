import streamlit as st

def mostrar_explicacion_regresion_lineal():
    st.title("ğŸ“ˆ ExplicaciÃ³n paso a paso de RegresiÃ³n Lineal Simple")

    st.markdown(r"""
    ### Â¿QuÃ© es la RegresiÃ³n Lineal Simple?

    Es un mÃ©todo estadÃ­stico para modelar la relaciÃ³n entre una variable dependiente \(Y\) y una variable independiente \(X\) usando una lÃ­nea recta.

    La fÃ³rmula general de la recta es:
    """)
    st.latex(r"Y = \beta_0 + \beta_1 X + \varepsilon")

    st.markdown(r"""
    Donde:  
    - \(\beta_0\) es el intercepto (ordenada al origen).  
    - \(\beta_1\) es la pendiente (cambio esperado en \(Y\) por unidad de cambio en \(X\)).  
    - \(\varepsilon\) es el tÃ©rmino de error o residual.

    ---

    ### Objetivo

    Encontrar los valores de \(\beta_0\) y \(\beta_1\) que minimicen el error cuadrÃ¡tico entre los valores observados y los predichos por el modelo.

    ---

    ### Paso 1: CÃ¡lculo de medias

    Se calcula la media de \(X\) y de \(Y\):

    \[
    \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \quad,\quad \bar{Y} = \frac{1}{n} \sum_{i=1}^n Y_i
    \]

    ---

    ### Paso 2: CÃ¡lculo de la pendiente \(\beta_1\)

    \[
    \beta_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}
    \]

    TambiÃ©n expresado como:

    \[
    \beta_1 = \frac{n \sum X_i Y_i - \sum X_i \sum Y_i}{n \sum X_i^2 - (\sum X_i)^2}
    \]

    ---

    ### Paso 3: CÃ¡lculo del intercepto \(\beta_0\)

    \[
    \beta_0 = \bar{Y} - \beta_1 \bar{X}
    \]

    ---

    ### Paso 4: EcuaciÃ³n final del modelo

    \[
    \hat{Y} = \beta_0 + \beta_1 X
    \]

    donde \(\hat{Y}\) es el valor predicho.

    ---

    ### Paso 5: EvaluaciÃ³n del modelo

    Se utilizan mÃ©tricas como:

    - **Error CuadrÃ¡tico Medio (MSE):**

    \[
    MSE = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
    \]

    - **Coeficiente de DeterminaciÃ³n \(R^2\):**

    \[
    R^2 = 1 - \frac{\sum (Y_i - \hat{Y}_i)^2}{\sum (Y_i - \bar{Y})^2}
    \]

    Que indica quÃ© proporciÃ³n de la variabilidad de \(Y\) es explicada por \(X\).

    ---

    ### InterpretaciÃ³n

    - Si \(\beta_1 > 0\), \(Y\) tiende a aumentar cuando \(X\) aumenta.  
    - Si \(\beta_1 < 0\), \(Y\) tiende a disminuir cuando \(X\) aumenta.  
    - Si \(R^2\) estÃ¡ cerca de 1, el modelo explica bien la relaciÃ³n.  
    - Si estÃ¡ cerca de 0, el modelo explica poco.

    ---

    ### Uso prÃ¡ctico

    Una vez calculados \(\beta_0\) y \(\beta_1\), puedes predecir \(Y\) para cualquier nuevo valor de \(X\) usando la fÃ³rmula del modelo.

    """)


def mostrar_explicacion_regresion_multiple():
    st.title("ğŸ“Š ExplicaciÃ³n paso a paso de RegresiÃ³n Lineal MÃºltiple")

    st.markdown(r"""
    ### Â¿QuÃ© es la RegresiÃ³n Lineal MÃºltiple?

    Es una extensiÃ³n de la regresiÃ³n lineal simple que modela la relaciÃ³n entre una variable dependiente \(Y\) y mÃºltiples variables independientes \(X_1, X_2, ..., X_n\).

    La fÃ³rmula general es:

    \[
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \varepsilon
    \]

    Donde:  
    - \(\beta_0\) es el intercepto.  
    - \(\beta_1, \beta_2, ..., \beta_n\) son los coeficientes de las variables independientes.  
    - \(\varepsilon\) es el tÃ©rmino de error.

    ---

    ### Objetivo

    Encontrar los coeficientes \(\beta\) que minimizan el error cuadrÃ¡tico entre los valores observados y los predichos, usando la suma de residuos al cuadrado.

    ---

    ### RepresentaciÃ³n matricial

    El modelo puede representarse como:

    \[
    \mathbf{Y} = \mathbf{X} \boldsymbol{\beta}
    \]

    Donde:

    \[
    \mathbf{Y} =
    \begin{bmatrix}
    Y_1 \\ Y_2 \\ \vdots \\ Y_m
    \end{bmatrix}
    ,\quad
    \mathbf{X} =
    \begin{bmatrix}
    1 & X_{11} & X_{12} & \cdots & X_{1n} \\
    1 & X_{21} & X_{22} & \cdots & X_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & X_{m1} & X_{m2} & \cdots & X_{mn}
    \end{bmatrix}
    ,\quad
    \boldsymbol{\beta} =
    \begin{bmatrix}
    \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_n
    \end{bmatrix}
    \]

    ---

    ### CÃ¡lculo de los coeficientes

    Usamos la ecuaciÃ³n normal:

    \[
    \boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
    \]

    Esta fÃ³rmula proporciona los coeficientes que minimizan la suma de errores cuadrados.

    ---

    ### InterpretaciÃ³n de coeficientes

    Cada \(\beta_i\) representa el cambio esperado en \(Y\) por unidad de cambio en \(X_i\), manteniendo las demÃ¡s variables constantes.

    ---

    ### Ejemplo prÃ¡ctico

    Supongamos que queremos predecir el precio de una casa segÃºn su tamaÃ±o (mÂ²) y nÃºmero de habitaciones:

    \[
    Precio = 31.04 + 1.4 \times TamaÃ±o + 2.5 \times Habitaciones
    \]

    Esto indica que por cada metro cuadrado adicional, el precio aumenta en 1.4 unidades monetarias, y por cada habitaciÃ³n adicional, aumenta en 2.5 unidades.

    ---

    ### Uso prÃ¡ctico

    La ecuaciÃ³n resultante puede usarse para hacer predicciones basadas en mÃºltiples variables independientes.

    """)

def mostrar_explicacion_k_means():
    st.title("ğŸ“Œ ExplicaciÃ³n paso a paso del algoritmo K-means")

    st.markdown(r"""
    K-means es un algoritmo de clustering que agrupa datos en \(k\) clusters basados en la distancia a centroides.

    ### Proceso general:

    1. Se elige el nÃºmero de clusters \(k\).
    2. Se inicializan los centroides (aleatoriamente o por mÃ©todos heurÃ­sticos).
    3. Cada punto se asigna al cluster cuyo centroide estÃ¡ mÃ¡s cercano (usualmente distancia euclidiana).
    4. Se recalculan los centroides como la media de los puntos asignados a cada cluster.
    5. Se repiten los pasos 3 y 4 hasta que las asignaciones no cambien (convergencia).
    
    ### CaracterÃ­sticas:

    - Puede trabajar con datos multidimensionales.
    - Es sensible a la inicializaciÃ³n de centroides.
    - No garantiza un Ã³ptimo global, pero suele converger rÃ¡pido.
    - Es comÃºn usar reducciÃ³n dimensional para visualizar clusters cuando hay muchas variables.

    ### Aplicaciones:

    - SegmentaciÃ³n de clientes.
    - Agrupamiento de documentos.
    - DetecciÃ³n de patrones en datos.
    """)
def mostrar_explicacion_k_modas():
    st.title("ExplicaciÃ³n de K-modas")
    st.markdown("""
    ## Â¿QuÃ© es K-modas?

    K-modas es una tÃ©cnica de clustering para datos categÃ³ricos. Similar a K-means, pero en lugar de usar medias y distancia euclidiana, usa modas (valores mÃ¡s frecuentes) y una medida de disimilitud basada en conteo de diferencias.

    ### Proceso bÃ¡sico:
    1. Se asignan los modos iniciales por cluster segÃºn la moda en los datos conocidos.
    2. Cada punto se asigna al cluster cuyo modo tiene menor nÃºmero de diferencias con el dato.
    3. Se recalculan los modos con las asignaciones actuales.
    4. Se repite hasta convergencia.
    5. Se imputan los valores faltantes usando el modo del cluster asignado.

    ### Aplicaciones comunes:
    - Datos categÃ³ricos puros.
    - SegmentaciÃ³n de clientes por atributos categÃ³ricos.
    - AnÃ¡lisis de comportamiento con variables no numÃ©ricas.

    ### Diferencias con K-means:
    - K-means usa medias y distancia euclidiana (numÃ©rico).
    - K-modas usa modas y distancia por conteo de diferencias (categÃ³rico).

    """)
def mostrar_explicacion_id3():
    st.title("ğŸŒŸ ExplicaciÃ³n del Ãrbol de DecisiÃ³n ID3")
    st.markdown(r"""
**Proceso de construcciÃ³n del Ãrbol de DecisiÃ³n ID3**

1. **IntroducciÃ³n**  
El algoritmo ID3 construye un Ã¡rbol de decisiÃ³n que clasifica datos buscando, en cada nodo, el atributo que maximiza la ganancia de informaciÃ³n (reducciÃ³n de entropÃ­a).

2. **EntropÃ­a**  
Mide la impureza o incertidumbre de un conjunto de datos:
""")
    st.latex(r"E(S) = - \sum_{i=1}^{c} p_i \log_k(p_i)")
    st.markdown(r"""
- \(S\): conjunto de datos.  
- \(c\): nÃºmero de clases.  
- \(p_i\): proporciÃ³n de ejemplos en la clase \(i\).

Si todas las instancias son de la misma clase, \(E(S)=0\). Si estÃ¡n uniformemente distribuidas, \(E(S)\) es mÃ¡ximo.

3. **Ganancia de InformaciÃ³n**  
Cuantifica cuÃ¡nto disminuye la entropÃ­a al dividir por un atributo \(A\):
""")
    st.latex(r"G(S,A) = E(S) - \sum_{v \in Valores(A)} \frac{|S_v|}{|S|}\,E(S_v)")
    st.markdown(r"""
- \(S_v\): subconjunto de \(S\) donde el atributo \(A\) toma el valor \(v\).

Se elige el atributo con **mayor** \(G(S,A)\) (o **menor** entropÃ­a condicional).

4. **Proceso recursivo**  
- Calcular \(E(S)\).  
- Para cada atributo restante, calcular \(G(S,A)\).  
- Crear un nodo con el mejor atributo y dividir \(S\) segÃºn sus valores.  
- Repetir en cada rama hasta que:  
  1. Todas las instancias queden en la misma clase (entropÃ­a = 0).  
  2. No queden atributos.

5. **ConstrucciÃ³n del Ã¡rbol**  
- **Nodos internos**: representan pruebas sobre atributos.  
- **Ramas**: representan los valores de esos atributos.  
- **Hojas**: contienen las clases finales.

6. **ExtracciÃ³n de reglas**  
Cada camino desde la raÃ­z hasta una hoja produce una regla del tipo:  
```
Si A1 = v1 y A2 = v2 â€¦, entonces Clase = c
```"""
)
