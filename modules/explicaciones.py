import streamlit as st

def mostrar_explicacion_regresion_lineal():
    st.title("游늳 Explicaci칩n paso a paso de Regresi칩n Lineal Simple")

    st.markdown(r"""
    ### 쯈u칠 es la Regresi칩n Lineal Simple?

    Es un m칠todo estad칤stico para modelar la relaci칩n entre una variable dependiente \(Y\) y una variable independiente \(X\) usando una l칤nea recta.

    La f칩rmula general de la recta es:
    """)
    st.latex(r"Y = \beta_0 + \beta_1 X + \varepsilon")

    st.markdown(r"""
    Donde:  
    - \(\beta_0\) es el intercepto (ordenada al origen).  
    - \(\beta_1\) es la pendiente (cambio esperado en \(Y\) por unidad de cambio en \(X\)).  
    - \(\varepsilon\) es el t칠rmino de error o residual.

    ---

    ### Objetivo

    Encontrar los valores de \(\beta_0\) y \(\beta_1\) que minimicen el error cuadr치tico entre los valores observados y los predichos por el modelo.

    ---

    ### Paso 1: C치lculo de medias

    Se calcula la media de \(X\) y de \(Y\):

    \[
    \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \quad,\quad \bar{Y} = \frac{1}{n} \sum_{i=1}^n Y_i
    \]

    ---

    ### Paso 2: C치lculo de la pendiente \(\beta_1\)

    \[
    \beta_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}
    \]

    Tambi칠n expresado como:

    \[
    \beta_1 = \frac{n \sum X_i Y_i - \sum X_i \sum Y_i}{n \sum X_i^2 - (\sum X_i)^2}
    \]

    ---

    ### Paso 3: C치lculo del intercepto \(\beta_0\)

    \[
    \beta_0 = \bar{Y} - \beta_1 \bar{X}
    \]

    ---

    ### Paso 4: Ecuaci칩n final del modelo

    \[
    \hat{Y} = \beta_0 + \beta_1 X
    \]

    donde \(\hat{Y}\) es el valor predicho.

    ---

    ### Paso 5: Evaluaci칩n del modelo

    Se utilizan m칠tricas como:

    - **Error Cuadr치tico Medio (MSE):**

    \[
    MSE = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
    \]

    - **Coeficiente de Determinaci칩n \(R^2\):**

    \[
    R^2 = 1 - \frac{\sum (Y_i - \hat{Y}_i)^2}{\sum (Y_i - \bar{Y})^2}
    \]

    Que indica qu칠 proporci칩n de la variabilidad de \(Y\) es explicada por \(X\).

    ---

    ### Interpretaci칩n

    - Si \(\beta_1 > 0\), \(Y\) tiende a aumentar cuando \(X\) aumenta.  
    - Si \(\beta_1 < 0\), \(Y\) tiende a disminuir cuando \(X\) aumenta.  
    - Si \(R^2\) est치 cerca de 1, el modelo explica bien la relaci칩n.  
    - Si est치 cerca de 0, el modelo explica poco.

    ---

    ### Uso pr치ctico

    Una vez calculados \(\beta_0\) y \(\beta_1\), puedes predecir \(Y\) para cualquier nuevo valor de \(X\) usando la f칩rmula del modelo.

    """)

def mostrar_explicacion_id3():
    st.title("Explicaci칩n del algoritmo 츼rbol de Decisi칩n ID3")
    st.markdown(r"""
    # Proceso de construcci칩n del 츼rbol de Decisi칩n ID3

    1. **Introducci칩n**  
    El algoritmo ID3 construye un 치rbol de decisi칩n que clasifica datos usando el criterio de m치xima ganancia de informaci칩n, basada en la entrop칤a.

    2. **Entrop칤a**  
    Mide la impureza o incertidumbre de un conjunto de datos.  
    F칩rmula:
    """)
    st.latex(r"Entrop칤a(S) = - \sum_{i=1}^c p_i \log_2(p_i)")
    st.markdown(r"""
    donde:  
    - \(S\) es el conjunto de datos,  
    - \(c\) es el n칰mero de clases,  
    - \(p_i\) es la proporci칩n de ejemplos en la clase \(i\).

    Si todos los datos pertenecen a una sola clase, la entrop칤a es 0 (conjunto puro).  
    Si las clases est치n distribuidas uniformemente, la entrop칤a es m치xima.

    3. **Ganancia de Informaci칩n**  
    Mide cu치nto reduce la entrop칤a un atributo al dividir los datos.  
    F칩rmula:
    """)
    st.latex(r"Ganancia(S, A) = Entrop칤a(S) - \sum_{v \in Valores(A)} \frac{|S_v|}{|S|} \cdot Entrop칤a(S_v)")
    st.markdown(r"""
    donde:  
    - \(S_v\) es el subconjunto de \(S\) donde el atributo \(A\) toma el valor \(v\).

    Elegimos el atributo con **m치xima ganancia** para dividir.

    4. **Proceso Recursivo**  
    Calcula la entrop칤a y ganancia para cada atributo.  
    Escoge el atributo con mayor ganancia para crear un nodo.  
    Divide el conjunto seg칰n los valores del atributo.  
    Repite recursivamente en cada subconjunto hasta que:  
    - Todos los ejemplos son de la misma clase (entrop칤a = 0).  
    - No quedan m치s atributos para dividir.

    5. **Construcci칩n del 츼rbol**  
    El nodo ra칤z es el atributo con mayor ganancia.  
    Cada rama corresponde a un valor del atributo.  
    Las hojas contienen las clases finales.

    6. **Extracci칩n de Reglas**  
    Cada camino desde la ra칤z hasta una hoja representa una regla.  
    La regla concatena las condiciones de cada nodo en el camino.  

    Ejemplo:  
    `Si Nivel acad칠mico = Mag칤ster y Estrato socioecon칩mico = Medio y 츼rea de estudio = Ingenier칤a, entonces Categor칤a = Titular`
    """)


def mostrar_explicacion_regresion_multiple():
    st.title("游늵 Explicaci칩n paso a paso de Regresi칩n Lineal M칰ltiple")

    st.markdown(r"""
    ### 쯈u칠 es la Regresi칩n Lineal M칰ltiple?

    Es una extensi칩n de la regresi칩n lineal simple que modela la relaci칩n entre una variable dependiente \(Y\) y m칰ltiples variables independientes \(X_1, X_2, ..., X_n\).

    La f칩rmula general es:

    \[
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \varepsilon
    \]

    Donde:  
    - \(\beta_0\) es el intercepto.  
    - \(\beta_1, \beta_2, ..., \beta_n\) son los coeficientes de las variables independientes.  
    - \(\varepsilon\) es el t칠rmino de error.

    ---

    ### Objetivo

    Encontrar los coeficientes \(\beta\) que minimizan el error cuadr치tico entre los valores observados y los predichos, usando la suma de residuos al cuadrado.

    ---

    ### Representaci칩n matricial

    El modelo puede representarse como:

    \[
    \mathbf{Y} = \mathbf{X} \boldsymbol{\beta}
    \]

    Donde:

    \[
    \mathbf{Y} =
    \begin{bmatrix}
    Y_1 \\ Y_2 \\ \vdots \\ Y_m
    \end{bmatrix}
    ,\quad
    \mathbf{X} =
    \begin{bmatrix}
    1 & X_{11} & X_{12} & \cdots & X_{1n} \\
    1 & X_{21} & X_{22} & \cdots & X_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & X_{m1} & X_{m2} & \cdots & X_{mn}
    \end{bmatrix}
    ,\quad
    \boldsymbol{\beta} =
    \begin{bmatrix}
    \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_n
    \end{bmatrix}
    \]

    ---

    ### C치lculo de los coeficientes

    Usamos la ecuaci칩n normal:

    \[
    \boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
    \]

    Esta f칩rmula proporciona los coeficientes que minimizan la suma de errores cuadrados.

    ---

    ### Interpretaci칩n de coeficientes

    Cada \(\beta_i\) representa el cambio esperado en \(Y\) por unidad de cambio en \(X_i\), manteniendo las dem치s variables constantes.

    ---

    ### Ejemplo pr치ctico

    Supongamos que queremos predecir el precio de una casa seg칰n su tama침o (m) y n칰mero de habitaciones:

    \[
    Precio = 31.04 + 1.4 \times Tama침o + 2.5 \times Habitaciones
    \]

    Esto indica que por cada metro cuadrado adicional, el precio aumenta en 1.4 unidades monetarias, y por cada habitaci칩n adicional, aumenta en 2.5 unidades.

    ---

    ### Uso pr치ctico

    La ecuaci칩n resultante puede usarse para hacer predicciones basadas en m칰ltiples variables independientes.

    """)

def mostrar_explicacion_k_means():
    st.title("Explicaci칩n del algoritmo K-means")
    st.markdown(r"""
    K-means es un algoritmo de clustering que particiona los datos en \(K\) grupos basados en la distancia a centroides.

    1. **Inicializaci칩n:**  
    Se eligen \(K\) centroides iniciales (aleatorios o seg칰n heur칤sticas).

    2. **Asignaci칩n:**  
    Cada punto se asigna al cluster con el centroide m치s cercano.

    3. **Actualizaci칩n:**  
    Se recalculan los centroides como la media de los puntos asignados.

    4. **Repetici칩n:**  
    Se repiten los pasos 2 y 3 hasta que los centroides no cambien significativamente.

    El objetivo es minimizar la suma de las distancias cuadr치ticas dentro de cada cluster.
    """)

