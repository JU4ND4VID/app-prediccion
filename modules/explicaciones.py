import streamlit as st










def mostrar_explicacion_id3():
    st.title("Explicaci칩n del algoritmo 츼rbol de Decisi칩n ID3")

    st.markdown(r"""
    # Proceso de construcci칩n del 츼rbol de Decisi칩n ID3

    1. **Introducci칩n**  
    El algoritmo ID3 construye un 치rbol de decisi칩n que clasifica datos usando el criterio de m치xima ganancia de informaci칩n, basada en la entrop칤a.

    2. **Entrop칤a**  
    Mide la impureza o incertidumbre de un conjunto de datos.  
    F칩rmula:
    """)
    st.latex("Entrop칤a(S) = - \\sum_{i=1}^c p_i \\log_2(p_i)")
    st.markdown(r"""
    donde:  
    - \(S\) es el conjunto de datos,  
    - \(c\) es el n칰mero de clases,  
    - \(p_i\) es la proporci칩n de ejemplos en la clase \(i\).

    Si todos los datos pertenecen a una sola clase, la entrop칤a es 0 (conjunto puro).  
    Si las clases est치n distribuidas uniformemente, la entrop칤a es m치xima.

    3. **Ganancia de Informaci칩n**  
    Mide cu치nto reduce la entrop칤a un atributo al dividir los datos.  
    F칩rmula:
    """)
    st.latex("Ganancia(S, A) = Entrop칤a(S) - \\sum_{v \\in Valores(A)} \\frac{|S_v|}{|S|} \\cdot Entrop칤a(S_v)")
    st.markdown(r"""
    donde:  
    - \(S_v\) es el subconjunto de \(S\) donde el atributo \(A\) toma el valor \(v\).

    Elegimos el atributo con **m치xima ganancia** para dividir.

    4. **Proceso Recursivo**  
    Calcula la entrop칤a y ganancia para cada atributo.  
    Escoge el atributo con mayor ganancia para crear un nodo.  
    Divide el conjunto seg칰n los valores del atributo.  
    Repite recursivamente en cada subconjunto hasta que:  
    - Todos los ejemplos son de la misma clase (entrop칤a = 0).  
    - No quedan m치s atributos para dividir.

    5. **Construcci칩n del 츼rbol**  
    El nodo ra칤z es el atributo con mayor ganancia.  
    Cada rama corresponde a un valor del atributo.  
    Las hojas contienen las clases finales.

    6. **Extracci칩n de Reglas**  
    Cada camino desde la ra칤z hasta una hoja representa una regla.  
    La regla concatena las condiciones de cada nodo en el camino.  

    Ejemplo:  
    `Si Nivel acad칠mico = Mag칤ster y Estrato socioecon칩mico = Medio y 츼rea de estudio = Ingenier칤a, entonces Categor칤a = Titular`
    """)


def mostrar_explicacion_regresion_lineal():
    st.title("游늳 Explicaci칩n paso a paso de Regresi칩n Lineal Simple")

    st.markdown("""
    ### 쯈u칠 es la Regresi칩n Lineal Simple?

    Es un m칠todo estad칤stico para modelar la relaci칩n entre una variable dependiente \(Y\) y una variable independiente \(X\) usando una l칤nea recta.

    La f칩rmula general de la recta es:
    """)
    st.latex(r"Y = \beta_0 + \beta_1 X + \varepsilon")

    st.markdown("""
    Donde:  
    - \(\beta_0\) es el intercepto (ordenada al origen).  
    - \(\beta_1\) es la pendiente (cambio esperado en \(Y\) por unidad de cambio en \(X\)).  
    - \(\varepsilon\) es el t칠rmino de error o residual.

    ---

    ### Objetivo

    Encontrar los valores de \(\beta_0\) y \(\beta_1\) que minimicen el error cuadr치tico entre los valores observados y los predichos por el modelo.

    ---

    ### Paso 1: C치lculo de medias

    Se calcula la media de \(X\) y de \(Y\):

    \[
    \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \quad,\quad \bar{Y} = \frac{1}{n} \sum_{i=1}^n Y_i
    \]

    ---

    ### Paso 2: C치lculo de la pendiente \(\beta_1\)

    \[
    \beta_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}
    \]

    Tambi칠n expresado como:

    \[
    \beta_1 = \frac{n \sum X_i Y_i - \sum X_i \sum Y_i}{n \sum X_i^2 - (\sum X_i)^2}
    \]

    ---

    ### Paso 3: C치lculo del intercepto \(\beta_0\)

    \[
    \beta_0 = \bar{Y} - \beta_1 \bar{X}
    \]

    ---

    ### Paso 4: Ecuaci칩n final del modelo

    \[
    \hat{Y} = \beta_0 + \beta_1 X
    \]

    donde \(\hat{Y}\) es el valor predicho.

    ---

    ### Paso 5: Evaluaci칩n del modelo

    Se utilizan m칠tricas como:

    - **Error Cuadr치tico Medio (MSE):**

    \[
    MSE = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
    \]

    - **Coeficiente de Determinaci칩n \(R^2\):**

    \[
    R^2 = 1 - \frac{\sum (Y_i - \hat{Y}_i)^2}{\sum (Y_i - \bar{Y})^2}
    \]

    Que indica qu칠 proporci칩n de la variabilidad de \(Y\) es explicada por \(X\).

    ---

    ### Interpretaci칩n

    - Si \(\beta_1 > 0\), \(Y\) tiende a aumentar cuando \(X\) aumenta.  
    - Si \(\beta_1 < 0\), \(Y\) tiende a disminuir cuando \(X\) aumenta.  
    - Si \(R^2\) est치 cerca de 1, el modelo explica bien la relaci칩n.  
    - Si est치 cerca de 0, el modelo explica poco.

    ---

    ### Uso pr치ctico

    Una vez calculados \(\beta_0\) y \(\beta_1\), puedes predecir \(Y\) para cualquier nuevo valor de \(X\) usando la f칩rmula del modelo.

    """)


def mostrar_explicacion_k_means():
    st.title("Explicaci칩n K-means")
    st.markdown("""
    K-means es un algoritmo de clustering que particiona los datos en K grupos basados en la distancia entre puntos y centroides.
    ...
    """)
    # Contenido detallado de la explicaci칩n K-means